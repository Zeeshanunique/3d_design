{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-to-Image Model Training for Clothing Generation\n",
        "\n",
        "This notebook fine-tunes Stable Diffusion on the H&M Clothes Descriptions dataset.\n",
        "\n",
        "**Dataset:** [wbensvage/clothes_desc](https://huggingface.co/datasets/wbensvage/clothes_desc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers diffusers accelerate datasets pillow numpy tqdm huggingface-hub\n",
        "\n",
        "# Set environment variable for better memory management\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "print(\"‚úÖ Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Training will be very slow on CPU!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "import pickle\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configuration\n",
        "\n",
        "**Note:** The configuration is optimized for Colab's T4 GPU (15GB). If you have more memory, you can increase `train_batch_size`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"pretrained_model\": \"runwayml/stable-diffusion-v1-5\",\n",
        "    \"dataset_name\": \"wbensvage/clothes_desc\",\n",
        "    \"output_dir\": \"./models/clothes-diffusion\",\n",
        "    \"cache_dir\": \"./data/cached_latents\",\n",
        "    \"resolution\": 512,\n",
        "    \"train_batch_size\": 1,  # Reduced for Colab GPU memory (T4 has ~15GB)\n",
        "    \"gradient_accumulation_steps\": 8,  # Increased to maintain effective batch size\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_train_steps\": 1000,  # Reduce to 100-200 for quick testing\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"mixed_precision\": \"fp16\",  # Use fp16 for faster training on GPU\n",
        "    \"seed\": 42,\n",
        "    \"preprocess_batch_size\": 8,  # Batch size for preprocessing\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClothesDataset(Dataset):\n",
        "    \"\"\"Dataset class for clothes images and text descriptions\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset, tokenizer, vae=None, size=512, device=\"cpu\", cached_latents=None, cached_texts=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "        \n",
        "        # Use cached latents if available (much faster!)\n",
        "        if cached_latents is not None and cached_texts is not None:\n",
        "            print(\"Using pre-cached latents (fast mode)\")\n",
        "            self.latents = cached_latents\n",
        "            self.texts = cached_texts\n",
        "            self.use_cache = True\n",
        "        else:\n",
        "            print(\"Using on-the-fly encoding (slower)\")\n",
        "            self.dataset = dataset\n",
        "            self.vae = vae\n",
        "            self.device = device\n",
        "            self.use_cache = False\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.use_cache:\n",
        "            return len(self.texts)\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.use_cache:\n",
        "            # Fast path: use pre-cached latents\n",
        "            latents = self.latents[idx]\n",
        "            text = self.texts[idx]\n",
        "        else:\n",
        "            # Slow path: encode on-the-fly\n",
        "            item = self.dataset[idx]\n",
        "            image = item['image']\n",
        "            text = item['text']\n",
        "            \n",
        "            # Convert to RGB if needed\n",
        "            if image.mode != \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            \n",
        "            # Resize image if needed\n",
        "            if image.size != (self.size, self.size):\n",
        "                image = image.resize((self.size, self.size), Image.LANCZOS)\n",
        "            \n",
        "            # Convert to tensor and normalize\n",
        "            image = np.array(image).astype(np.float32) / 255.0\n",
        "            image = (image - 0.5) / 0.5  # Normalize to [-1, 1]\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1)  # CHW format\n",
        "            \n",
        "            # Encode image to latent space using VAE\n",
        "            with torch.no_grad():\n",
        "                image_batch = image.unsqueeze(0).to(self.device)\n",
        "                latents = self.vae.encode(image_batch).latent_dist.sample()\n",
        "                latents = latents * self.vae.config.scaling_factor\n",
        "                latents = latents.squeeze(0)\n",
        "        \n",
        "        # Tokenize text\n",
        "        text_inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"latents\": latents.cpu() if not self.use_cache else latents,\n",
        "            \"input_ids\": text_inputs.input_ids.flatten(),\n",
        "        }\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"Collate function for DataLoader\"\"\"\n",
        "    latents = [example[\"latents\"] for example in examples]\n",
        "    input_ids = [example[\"input_ids\"] for example in examples]\n",
        "    \n",
        "    latents = torch.stack(latents)\n",
        "    latents = latents.to(memory_format=torch.contiguous_format).float()\n",
        "    \n",
        "    input_ids = torch.stack(input_ids)\n",
        "    \n",
        "    return {\n",
        "        \"latents\": latents,\n",
        "        \"input_ids\": input_ids,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_dataset():\n",
        "    \"\"\"Pre-process dataset by encoding all images to latent space\"\"\"\n",
        "    \n",
        "    print(\"Loading VAE encoder...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"], subfolder=\"vae\"\n",
        "    )\n",
        "    vae = vae.to(device)\n",
        "    vae.eval()\n",
        "    vae.requires_grad_(False)\n",
        "    \n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
        "    \n",
        "    # Check if cache already exists\n",
        "    latents_path = os.path.join(CONFIG[\"cache_dir\"], \"latents.pt\")\n",
        "    texts_path = os.path.join(CONFIG[\"cache_dir\"], \"texts.pkl\")\n",
        "    \n",
        "    if os.path.exists(latents_path) and os.path.exists(texts_path):\n",
        "        print(f\"Cache already exists at {CONFIG['cache_dir']}\")\n",
        "        print(\"Loading cached latents...\")\n",
        "        cached_latents = torch.load(latents_path)\n",
        "        with open(texts_path, \"rb\") as f:\n",
        "            cached_texts = pickle.load(f)\n",
        "        print(f\"Loaded {len(cached_texts)} pre-cached latents!\")\n",
        "        return cached_latents, cached_texts\n",
        "    \n",
        "    # Process images in batches\n",
        "    all_latents = []\n",
        "    all_texts = []\n",
        "    \n",
        "    print(f\"Encoding {len(dataset)} images to latent space...\")\n",
        "    print(f\"This may take a few minutes, but will speed up training significantly!\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(dataset), CONFIG[\"preprocess_batch_size\"])):\n",
        "            batch_end = min(i + CONFIG[\"preprocess_batch_size\"], len(dataset))\n",
        "            batch_images = []\n",
        "            batch_texts = []\n",
        "            \n",
        "            # Prepare batch\n",
        "            for j in range(i, batch_end):\n",
        "                item = dataset[j]\n",
        "                image = item['image']\n",
        "                text = item['text']\n",
        "                \n",
        "                # Convert to RGB if needed\n",
        "                if image.mode != \"RGB\":\n",
        "                    image = image.convert(\"RGB\")\n",
        "                \n",
        "                # Resize if needed\n",
        "                if image.size != (CONFIG[\"resolution\"], CONFIG[\"resolution\"]):\n",
        "                    image = image.resize((CONFIG[\"resolution\"], CONFIG[\"resolution\"]), Image.LANCZOS)\n",
        "                \n",
        "                # Convert to tensor and normalize\n",
        "                image_array = np.array(image).astype(np.float32) / 255.0\n",
        "                image_array = (image_array - 0.5) / 0.5  # Normalize to [-1, 1]\n",
        "                image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)  # CHW format\n",
        "                \n",
        "                batch_images.append(image_tensor)\n",
        "                batch_texts.append(text)\n",
        "            \n",
        "            # Stack batch\n",
        "            batch_tensor = torch.stack(batch_images).to(device)\n",
        "            \n",
        "            # Encode to latent space\n",
        "            latents = vae.encode(batch_tensor).latent_dist.sample()\n",
        "            latents = latents * vae.config.scaling_factor\n",
        "            \n",
        "            # Move to CPU and store\n",
        "            all_latents.append(latents.cpu())\n",
        "            all_texts.extend(batch_texts)\n",
        "    \n",
        "    # Concatenate all latents\n",
        "    print(\"Concatenating latents...\")\n",
        "    all_latents = torch.cat(all_latents, dim=0)\n",
        "    \n",
        "    # Save cached data\n",
        "    print(f\"Saving cached latents to {CONFIG['cache_dir']}...\")\n",
        "    torch.save(all_latents, latents_path)\n",
        "    \n",
        "    with open(texts_path, \"wb\") as f:\n",
        "        pickle.dump(all_texts, f)\n",
        "    \n",
        "    print(f\"‚úÖ Pre-processing complete!\")\n",
        "    print(f\"Cached {len(all_texts)} images\")\n",
        "    print(f\"Latent shape: {all_latents.shape}\")\n",
        "    \n",
        "    # Delete VAE to free memory\n",
        "    del vae\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return all_latents, all_texts\n",
        "\n",
        "# Run preprocessing\n",
        "cached_latents, cached_texts = preprocess_dataset()\n",
        "\n",
        "# Clear GPU cache after preprocessing\n",
        "if torch.cuda.is_available():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU cache cleared after preprocessing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Load Models and Setup Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU cache before loading models\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    print(\"üßπ Cleared GPU cache before loading models\")\n",
        "\n",
        "# Initialize accelerator\n",
        "use_cpu = not torch.cuda.is_available()\n",
        "mixed_precision = \"no\" if use_cpu else CONFIG[\"mixed_precision\"]\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "    mixed_precision=mixed_precision,\n",
        ")\n",
        "\n",
        "print(f\"Accelerator device: {accelerator.device}\")\n",
        "print(f\"Mixed precision: {mixed_precision}\")\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "# Load tokenizer and text encoder\n",
        "print(\"Loading tokenizer and text encoder...\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    CONFIG[\"pretrained_model\"], subfolder=\"tokenizer\"\n",
        ")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    CONFIG[\"pretrained_model\"], subfolder=\"text_encoder\"\n",
        ")\n",
        "\n",
        "# Load scheduler and UNet\n",
        "print(\"Loading scheduler and UNet...\")\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(\n",
        "    CONFIG[\"pretrained_model\"], subfolder=\"scheduler\"\n",
        ")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    CONFIG[\"pretrained_model\"], subfolder=\"unet\"\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if CONFIG[\"gradient_checkpointing\"]:\n",
        "    unet.enable_gradient_checkpointing()\n",
        "    print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "# Freeze text encoder\n",
        "# IMPORTANT: Keep text encoder on CPU to save GPU memory - we'll move it to GPU only when needed\n",
        "text_encoder.requires_grad_(False)\n",
        "text_encoder.eval()\n",
        "# Keep text encoder on CPU - we'll move inputs to CPU for encoding\n",
        "print(\"üìå Text encoder kept on CPU to save GPU memory\")\n",
        "\n",
        "# Clear cache after loading models\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ Models loaded successfully! GPU cache cleared.\")\n",
        "else:\n",
        "    print(\"‚úÖ Models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Create Dataset and DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (just for compatibility, we use cached latents)\n",
        "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
        "\n",
        "# Create dataset wrapper using cached latents\n",
        "train_dataset = ClothesDataset(\n",
        "    dataset, tokenizer, cached_latents=cached_latents, cached_texts=cached_texts\n",
        ")\n",
        "\n",
        "# Create dataloader with pin_memory=False to save memory\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"train_batch_size\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Reduced to 0 to save memory\n",
        "    pin_memory=False,  # Don't pin memory to save GPU memory\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Number of batches: {len(train_dataloader)}\")\n",
        "\n",
        "# Note: Gradient checkpointing is already enabled, which helps with memory\n",
        "# Attention slicing is for inference pipelines, not needed during training\n",
        "print(\"‚úÖ Dataset and DataLoader ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Initialize Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    unet.parameters(),\n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "unet, optimizer, train_dataloader = accelerator.prepare(\n",
        "    unet, optimizer, train_dataloader\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Optimizer initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9.5: Memory Optimization (Important!)\n",
        "\n",
        "**If you get OutOfMemoryError, restart the runtime and run all cells from Step 1 again.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory optimization: Clear all unnecessary variables\n",
        "import gc\n",
        "\n",
        "# Delete VAE if it was loaded (we don't need it anymore)\n",
        "if 'vae' in locals():\n",
        "    del vae\n",
        "\n",
        "# Delete dataset object to free memory (we have cached latents)\n",
        "if 'dataset' in locals():\n",
        "    del dataset\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")\n",
        "    print(\"‚úÖ Memory optimized before training\")\n",
        "    \n",
        "    # Check if we have enough free memory\n",
        "    free_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9\n",
        "    if free_memory < 0.5:\n",
        "        print(\"‚ö†Ô∏è  WARNING: Very little free GPU memory! Training may fail.\")\n",
        "        print(\"   Consider: 1) Restart runtime, 2) Reduce max_train_steps, 3) Use Colab Pro\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"üöÄ Starting training...\")\n",
        "if use_cpu:\n",
        "    print(\"‚ö†Ô∏è  Training on CPU - this will be slow. Consider using GPU.\")\n",
        "\n",
        "# Clear cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "global_step = 0\n",
        "save_interval = max(100, CONFIG[\"max_train_steps\"] // 10)  # Save every 100 steps or 10% of total\n",
        "\n",
        "unet.train()\n",
        "progress_bar = tqdm(total=CONFIG[\"max_train_steps\"], desc=\"Training\")\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    with accelerator.accumulate(unet):\n",
        "        # Get latents (already encoded by dataset)\n",
        "        latents = batch[\"latents\"].to(accelerator.device)\n",
        "        \n",
        "        # Sample noise\n",
        "        noise = torch.randn_like(latents)\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
        "        ).long()\n",
        "        \n",
        "        # Add noise to latents\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "        \n",
        "        # Get text embeddings (with no_grad to save memory)\n",
        "        # Text encoder is on CPU, so we need to move input_ids to CPU\n",
        "        with torch.no_grad():\n",
        "            # Ensure input_ids are on CPU (they might have been moved to GPU by accelerator)\n",
        "            input_ids_cpu = batch[\"input_ids\"].cpu()\n",
        "            # Encode on CPU where text encoder is\n",
        "            encoder_hidden_states = text_encoder(input_ids_cpu)[0]\n",
        "            # Move result to GPU for training\n",
        "            encoder_hidden_states = encoder_hidden_states.to(accelerator.device)\n",
        "            del input_ids_cpu\n",
        "        \n",
        "        # Predict noise\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "        \n",
        "        # Backward pass\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Clear intermediate variables to free memory immediately\n",
        "        del model_pred, noisy_latents, encoder_hidden_states\n",
        "        del latents, noise, timesteps\n",
        "    \n",
        "    progress_bar.update(1)\n",
        "    progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"step\": global_step})\n",
        "    global_step += 1\n",
        "    \n",
        "    # Periodic cache clearing and checkpoint saving\n",
        "    if global_step % 25 == 0:  # Clear cache more frequently (every 25 steps)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    \n",
        "    if global_step % save_interval == 0:\n",
        "        checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint-{global_step}\")\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        unwrapped_unet = accelerator.unwrap_model(unet)\n",
        "        unwrapped_unet.save_pretrained(checkpoint_dir)\n",
        "        print(f\"\\nüíæ Saved checkpoint at step {global_step} to {checkpoint_dir}\")\n",
        "        # Clear cache after saving\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    if global_step >= CONFIG[\"max_train_steps\"]:\n",
        "        break\n",
        "\n",
        "progress_bar.close()\n",
        "print(\"\\n‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving final model to {CONFIG['output_dir']}...\")\n",
        "\n",
        "# Save UNet\n",
        "unwrapped_unet = accelerator.unwrap_model(unet)\n",
        "unwrapped_unet.save_pretrained(CONFIG[\"output_dir\"])\n",
        "\n",
        "# Save tokenizer, text encoder, VAE, and scheduler\n",
        "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
        "text_encoder.save_pretrained(os.path.join(CONFIG[\"output_dir\"], \"text_encoder\"))\n",
        "\n",
        "# Load and save VAE\n",
        "vae = AutoencoderKL.from_pretrained(CONFIG[\"pretrained_model\"], subfolder=\"vae\")\n",
        "vae.save_pretrained(os.path.join(CONFIG[\"output_dir\"], \"vae\"))\n",
        "\n",
        "noise_scheduler.save_pretrained(os.path.join(CONFIG[\"output_dir\"], \"scheduler\"))\n",
        "\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "print(f\"Model location: {CONFIG['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Download Model (for use on your local machine)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a zip file of the model\n",
        "import shutil\n",
        "\n",
        "model_zip = \"clothes-diffusion-model.zip\"\n",
        "print(f\"Creating zip file: {model_zip}...\")\n",
        "\n",
        "# Remove old zip if exists\n",
        "if os.path.exists(model_zip):\n",
        "    os.remove(model_zip)\n",
        "\n",
        "# Create zip\n",
        "shutil.make_archive(\"clothes-diffusion-model\", \"zip\", CONFIG[\"output_dir\"])\n",
        "\n",
        "print(f\"‚úÖ Zip file created: {model_zip}\")\n",
        "print(f\"File size: {os.path.getsize(model_zip) / 1e9:.2f} GB\")\n",
        "print(\"\\nüì• Download the zip file from Colab's file browser (left sidebar)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Test Generation (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "print(\"Loading trained model for testing...\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    CONFIG[\"output_dir\"],\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    safety_checker=None,  # Disable safety checker for clothing images\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Generate a test image\n",
        "test_prompt = \"Black boxer briefs with elasticated waist\"\n",
        "print(f\"Generating image for: '{test_prompt}'...\")\n",
        "\n",
        "with torch.autocast(device) if device == \"cuda\" else torch.no_grad():\n",
        "    image = pipe(\n",
        "        test_prompt,\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "        height=512,\n",
        "        width=512,\n",
        "    ).images[0]\n",
        "\n",
        "# Save test image\n",
        "test_output_path = \"test_generation.png\"\n",
        "image.save(test_output_path)\n",
        "print(f\"‚úÖ Test image saved to: {test_output_path}\")\n",
        "image\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
